# use algorithm to find and narrow it down the solution from a large sum of data #

import numpy as np

p_rain = 3.0/10.0
p_gym = 3.0/5.0
p_rain_and_gym = (p_rain) * (p_gym)
print(p_rain_and_gym)

### naive bayes #####

import numpy as np


p_positive_given_disease = 0.99
p_disease = 1.0/100000.0

p_a = p_positive_given_disease * p_disease
p_b = (1 - p_disease) * (1 - p_positive_given_disease)
p_positive = p_a + p_b

p_disease_given_positive = (
  p_disease * p_positive_given_disease)/p_positive
print(p_disease_given_positive)

### example 2 ###

import numpy as np

a = 'spam'
b = 'enhancement'

p_spam = 0.2
p_enhancement_given_spam = 0.05
p_enhancement = 0.05 * 0.2 + 0.001 * (1 - 0.2)
p_spam_enhancement = p_enhancement_given_spam * p_spam / p_enhancement

print p_spam_enhancement

####### find the probability of each string ####

from reviews import neg_counter, pos_counter

review = "This crib was amazing"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

pos_probability = 1
neg_probability = 1

review_words = review.split()
for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  pos_probability *= word_in_pos / total_pos
  neg_probability *= word_in_neg / total_neg

print(pos_probability)
print(neg_probability)

##### smoothing: to prevent multiplication of null in the equation #####

when error in strings were found 

from reviews import neg_counter, pos_counter

review = "This cribb was amazing"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

pos_probability = 1
neg_probability = 1

review_words = review.split()

for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  
  pos_probability *= (word_in_pos +1)/ (total_pos + len(pos_counter))
  neg_probability *= (word_in_neg +1)/ (total_neg + len(neg_counter))
  
print(pos_probability)
print(neg_probability)

### classify ####

from reviews import neg_counter, pos_counter

review = "This crib was too expensive"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

pos_probability = 1
neg_probability = 1

review_words = review.split()

for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  
  pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))
  neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))

final_pos = pos_probability * percent_pos
final_neg = neg_probability * percent_neg

if final_pos > final_neg:
  print('The review is positive')
else: 
  print('The review is negative')

##### scikit-learn module #####

from reviews import neg_list, pos_list
from sklearn.feature_extraction.text import CountVectorizer

review = "This crib was amazing"

counter = CountVectorizer()
counter.fit(neg_list + pos_list)
print(counter.vocabulary_)

review_counts = counter.transform([review])
print(review_counts.toarray())

training_counts = counter.transform(neg_list + pos_list)

#### scikit-learn 2 ###

from reviews import counter, training_counts
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

review = "This crib was great amazing and wonderful"
review_counts = counter.transform([review])

classifier = MultinomialNB()
training_labels = [0] * 1000 + [1] * 1000

classifier.fit(training_counts, training_labels)
print(classifier.predict(review_counts))
print(classifier.predict_proba(review_counts))

#### final example #####

from reviews import baby_counter, baby_training, instant_video_counter, instant_video_training, video_game_counter, video_game_training
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

review = "this game was awesome"

baby_review_counts = baby_counter.transform([review])
instant_video_review_counts = instant_video_counter.transform([review])
video_game_review_counts = video_game_counter.transform([review])

baby_classifier = MultinomialNB()
instant_video_classifier = MultinomialNB()
video_game_classifier = MultinomialNB()

baby_labels = [0] * 1000 + [1] * 1000
instant_video_labels = [0] * 1000 + [1] * 1000
video_game_labels = [0] * 1000 + [1] * 1000


baby_classifier.fit(baby_training, baby_labels)
instant_video_classifier.fit(instant_video_training, instant_video_labels)
video_game_classifier.fit(video_game_training, video_game_labels)

print("Baby training set: " +str(baby_classifier.predict_proba(baby_review_counts)))
print("Amazon Instant Video training set: " + str(instant_video_classifier.predict_proba(instant_video_review_counts)))
print("Video Games training set: " + str(video_game_classifier.predict_proba(video_game_review_counts)))

### project ###

from sklearn.datasets import fetch_20newsgroups
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

train_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware','rec.sport.hockey'], subset='train', shuffle = True, random_state = 108)

#print(emails.target_names)
#print(emails.data[5])
#print(emails.target[5])

test_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware','rec.sport.hockey'], subset='test', shuffle = True, random_state = 108)

# let data become a list and count words
counter = CountVectorizer()
counter.fit(test_emails.data + train_emails.data)
train_counts = counter.transform(train_emails.data)
test_counts = counter.transform(test_emails.data)

#  classifier (generate probability)
classifier = MultinomialNB()
classifier.fit(train_counts, train_emails.target)
print(classifier.score(test_counts, test_emails.target))

#### project for theorem ####

import numpy as np

#A = knows materials
#B = answered correctly

#p_A
p_knows_material = 0.6
p_wrong_knows_material = 0.15
#p_BA
p_correct_knows_material = 1- 0.15
p_not_know_correct = 0.2

#calculation from two correct scenarios
#p_B
p_correct = (p_correct_knows_material * p_knows_material) + p_not_know_correct * (1 - p_knows_material)

#p_AB
p_knows_material_correct = (p_correct_knows_material * p_knows_material ) / p_correct

print(p_knows_material_correct)